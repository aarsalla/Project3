{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\obrie\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1a914c09bc33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "import pymongo\n",
    "\n",
    "# SQL Alchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, Integer, String, Float\n",
    "Base = declarative_base()\n",
    "\n",
    "# PyMySQL \n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "\n",
    "# Set path \n",
    "import sys\n",
    "\n",
    "# Config variables\n",
    "# from config import local_gwsis_dbname, local_gwsis_dbuser, local_gwsis_dbpwd\n",
    "from config import remote_db_endpoint, remote_db_port\n",
    "from config import remote_gwsis_dbname, remote_gwsis_dbuser, remote_gwsis_dbpwd\n",
    "\n",
    "\n",
    "import datetime\n",
    "from time import gmtime, strftime, strptime\n",
    "from datetime import timedelta\n",
    "import requests\n",
    "import json\n",
    "\n",
    "#Machine Learning Regressions/Clustering\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#Nueral Networks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to AWS database\n",
    "\n",
    "aengine = create_engine(f\"mysql://{remote_gwsis_dbuser}:{remote_gwsis_dbpwd}@{remote_db_endpoint}:{remote_db_port}/{remote_gwsis_dbname}\")\n",
    "aconn = aengine.connect()\n",
    "aengine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling in tables\n",
    "\n",
    "master_15_day = pd.read_sql('SELECT * FROM master_15_day_2',aconn)\n",
    "master_30_day = pd.read_sql('SELECT * FROM master_30_day_2',aconn)\n",
    "master_45_day = pd.read_sql('SELECT * FROM master_45_day_2',aconn)\n",
    "master_60_day = pd.read_sql('SELECT * FROM master_60_day_2',aconn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "### Predicting Future Stock Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove string columns and N/A values\n",
    "\n",
    "next_day_45_data = master_45_day.drop(['index','Stock Name','Reported Date','45 Days'],axis=1)\n",
    "next_day_45_data.loc[next_day_45_data['45 Days Later Stock Price'] == 'N/A','45 Days Later Stock Price'] = np.nan\n",
    "next_day_45_data.loc[next_day_45_data['Next Day Prices'] == 'N/A','Next Day Prices'] = np.nan\n",
    "next_day_45_data = next_day_45_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all columns numeric\n",
    "\n",
    "for i in next_day_45_data.columns:\n",
    "    next_day_45_data[i]= pd.to_numeric(next_day_45_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment line below to rest index\n",
    "#next_day_45_data = next_day_45_data.reset_index()\n",
    "\n",
    "# add percent change column to table\n",
    "percent_change = []\n",
    "for i in range(0,len(next_day_45_data)):\n",
    "    percent = (next_day_45_data['45 Days Later Stock Price'][i] - next_day_45_data['Next Day Prices'][i])/ next_day_45_data['Next Day Prices'][i]\n",
    "    percent_change.append(percent)\n",
    "next_day_45_data['% Change'] = percent_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create correlation heat map\n",
    "\n",
    "correlation_45 = next_day_45_data[['45 Days Later Stock Price', 'Next Day Prices', 'Chaikin A/D line','ADX',\n",
    "                                  'Aroon Up', 'Aroon Down','Lower Band (5)', 'Middle Band (5)','Upper Band (5)',\n",
    "                                   'CCI', 'Earnings Per Share','Forecasted Earnings Per Share', '% Surprise',\n",
    "                                  'EMA (20 Day)','MACD','OBV','RSI','20 Day Moving Average','5 Day Moving Average',\n",
    "                                  'SlowD', 'SlowK']]\n",
    "\n",
    "plt.figure(figsize = (12,7))\n",
    "ax = sns.heatmap(correlation_45.corr(),cmap='coolwarm',linewidths=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose X variables to test\n",
    "\n",
    "X = next_day_45_data[['Next Day Prices', '20 Day Moving Average','5 Day Moving Average',\n",
    "                  'Lower Band (5)','% Surprise']]\n",
    "y = next_day_45_data['45 Days Later Stock Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "training_score = model.score(X_train, y_train)\n",
    "testing_score = model.score(X_test, y_test)\n",
    "testing_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see p-values of variables and weights of coefs\n",
    "X2 = sm.add_constant(X)\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test results of model, save to csv and \"invest\" to see ROI\n",
    "results = model.predict(X_train)\n",
    "df_results = pd.DataFrame({'Predictions': results, 'Actual': y_train, 'Stock Price': X_train['Next Day Prices']})\n",
    "df_results.to_csv('results_45_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliars\n",
    "next_day_45_data.loc[next_day_45_data['% Surprise'] <-65,'% Surprise'] = np.nan\n",
    "next_day_45_data.loc[next_day_45_data['% Surprise'] >100,'% Surprise'] = np.nan\n",
    "outliars_removed = next_day_45_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try training again\n",
    "X = outliars_removed[['Next Day Prices', '20 Day Moving Average','5 Day Moving Average',\n",
    "                  'Lower Band (5)','% Surprise']]\n",
    "y = outliars_removed['45 Days Later Stock Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "training_score = model.score(X_train, y_train)\n",
    "testing_score = model.score(X_test, y_test)\n",
    "testing_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(X_train)\n",
    "df_results = pd.DataFrame({'Predictions': results, 'Actual': y_train, 'Stock Price': X_train['Next Day Prices']})\n",
    "df_results.to_csv('results_45_final_outliars_removed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "### Predicting Percent Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating ratio columns to compare percent change (i.e. don't put more weight on a stock that has a higher stock price)\n",
    "x_45 = next_day_45_data[['% Change','Next Day Prices', 'Chaikin A/D line','ADX',\n",
    "                                  'Aroon Up', 'Aroon Down','Lower Band (5)', 'Middle Band (5)','Upper Band (5)',\n",
    "                                  'Lower Band', 'Middle Band','Upper Band',\n",
    "                                   'CCI', 'Earnings Per Share','Forecasted Earnings Per Share', '% Surprise',\n",
    "                                  'EMA (20 Day)','MACD','OBV','RSI','20 Day Moving Average','5 Day Moving Average',\n",
    "                                  'SlowD', 'SlowK']]\n",
    "for i in x_45.columns:\n",
    "    if i != '% Change':\n",
    "        y = i + ' Ratio'\n",
    "        x_45[y] = x_45[i]/x_45['Next Day Prices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix with % Change\n",
    "correlation_45 = x[['% Change', 'Next Day Prices', 'Chaikin A/D line Ratio','ADX Ratio',\n",
    "                                  'Aroon Up Ratio', 'Aroon Down Ratio','Lower Band (5) Ratio', \n",
    "                                'Middle Band (5) Ratio','Upper Band (5) Ratio',\n",
    "                                   'CCI Ratio', 'Earnings Per Share Ratio','Forecasted Earnings Per Share Ratio', \n",
    "                            '% Surprise Ratio','EMA (20 Day) Ratio','MACD Ratio','OBV Ratio','RSI Ratio',\n",
    "                    '20 Day Moving Average Ratio','5 Day Moving Average Ratio','SlowD Ratio', 'SlowK Ratio']]\n",
    "\n",
    "plt.figure(figsize = (12,7))\n",
    "ax = sns.heatmap(correlation_45.corr(),cmap='coolwarm',linewidths=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model, score is so low, so I won't test in a csv\n",
    "X = next_day_45_data[['EMA (20 Day)','5 Day Moving Average',\n",
    "                  'Upper Band (5)','Aroon Up', 'MACD']]\n",
    "y = next_day_45_data['% Change']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "training_score = model.score(X_train, y_train)\n",
    "testing_score = model.score(X_test, y_test)\n",
    "testing_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "### Predicting if stock price will be higher or lower in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column, 1 if stock went up, 0 if it went down\n",
    "up_down = []\n",
    "for i in range(0,len(next_day_45_data)):\n",
    "    if next_day_45_data['45 Days Later Stock Price'][i]> next_day_45_data['Next Day Prices'][i]:\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 0\n",
    "    up_down.append(x)\n",
    "\n",
    "next_day_45_data['Up_Down'] = up_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training logistic regression\n",
    "classifier = LogisticRegression()\n",
    "X = next_day_45_data[['Next Day Prices','5 Day Moving Average',\n",
    "                  'Aroon Down','MACD','MACD_Signal']]\n",
    "y = next_day_45_data['Up_Down']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
    "classifier.fit(X_train, y_train)\n",
    "print(f\"Training Data Score: {classifier.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find probabilites of each stock going up or down. Create 3 csvs and combine in excel to see ROI. Will need index\n",
    "# from X_test to get 45 days later stock price in index.csv. I can go over this in person if its confusing/you can\n",
    "# probably do this formating in python, was just easier for me to do it in excel\n",
    "# bad job of predicting (pretty much just predicts everything goes up since there are more stocks that go up than down)\n",
    "probs = classifier.predict_proba(X_test)\n",
    "down = []\n",
    "up = []\n",
    "for i in range(0,len(probs)):\n",
    "    up.append(probs[i][1])\n",
    "    down.append(probs[i][0])\n",
    "df_up = pd.DataFrame({'Up': up, 'Down': down})\n",
    "df_up.to_csv('logistic_up_down_final.csv')\n",
    "X_test.to_csv('logistic_final.csv')\n",
    "next_day_45_data.to_csv('index.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nueral Network\n",
    "### Predicting Stock Price using Nueral Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next_day_45_data[['Next Day Prices', '20 Day Moving Average','5 Day Moving Average',\n",
    "                  'Lower Band (5)','% Surprise']]\n",
    "y = next_day_45_data['45 Days Later Stock Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScater model and fit it to the training data\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting model up, creating layers\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(15, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(30, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(30, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting model\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]\n",
    "NN_model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking performance\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "predictions = NN_model.predict(X_test)\n",
    "predictions\n",
    "predictions_list = []\n",
    "for i in range(0,len(predictions)):\n",
    "    predictions_list.append(predictions[i][0])\n",
    "df_nn = pd.DataFrame({'Predictions':predictions_list,'Actual': y_test, 'Starting Price': X_test['Next Day Prices']})\n",
    "df_nn.to_csv('NN_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up KMeans \n",
    "kmeans = KMeans(n_clusters=3)\n",
    "X = next_day_45_data[['Next Day Prices', '20 Day Moving Average','5 Day Moving Average',\n",
    "                  'Lower Band (5)','% Surprise']]\n",
    "y = next_day_45_data['45 Days Later Stock Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closer to 0 the better\n",
    "kmeans.score(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
